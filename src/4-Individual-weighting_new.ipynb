{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\MAD4AG\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd C:\\MAD4AG\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aglar\\AppData\\Local\\anaconda3\\envs\\pycharm\\lib\\site-packages\\geopandas\\_compat.py:115: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import geopandas\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "file_name = f'./dbs/intermediate/stops_1_new.parquet'\n",
    "\n",
    "#Read each batchs file\n",
    "df = pd.read_parquet(file_name)\n",
    "# df = df[df.holiday_s != 1]\n",
    "# df = df[df.weekday_s == 1]\n",
    "# df = df.drop(['holiday_s', 'weekday_s'], axis=1)\n",
    "df.drop_duplicates(subset=['uid', 'cluster'], keep='first', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# read home clusters\n",
    "\n",
    "df_h = pd.read_parquet(f'./dbs/intermediate/home_inference.parquet')\n",
    "df_h.drop_duplicates(subset='uid', keep='first', inplace=True)\n",
    "df_h['home_potential'] = 1\n",
    "df_h['work_potential'] = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# read work clusters, keep only ppl having home locations\n",
    "\n",
    "df_w = pd.read_parquet(f'./dbs/intermediate/work_inference.parquet')\n",
    "df_w.drop_duplicates(subset='uid', keep='first', inplace=True)\n",
    "df_w['home_potential'] = 0\n",
    "\n",
    "ppl_having_home = df_h.uid.unique()\n",
    "df_w = df_w[df_w.uid.isin(ppl_having_home)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# concat home and work clusters and create activity table\n",
    "\n",
    "df_act = pd.concat([df_h[['uid', 'cluster', 'home_potential', 'work_potential']], df_w[['uid', 'cluster', 'home_potential', 'work_potential']]],axis=0, ignore_index=True).sort_values(by='uid')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# add lat and long information of the clusters\n",
    "\n",
    "df_act = pd.merge(df_act, df[['uid', 'cluster', 'cluster_lat', 'cluster_lng' ]], on=['uid', 'cluster'], how=\"left\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJCS[\"SWEREF99 TM\",GEOGCS[\"SWEREF99\",DATUM[\"SWEREF99\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6619\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",15],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3006\"]]\n",
      "epsg:4326\n"
     ]
    }
   ],
   "source": [
    "# read population by deso zones\n",
    "\n",
    "DeSO = geopandas.read_file(\n",
    "    f'C:/Synthetic_population_new/caglar/synthetic_sweden/input/deso_statistik_shp/Bef_Alder_region.shp')\n",
    "\n",
    "DeSO.loc[:, 'adult_pop'] = DeSO.loc[:, [ 'Ald20_24', 'Ald25_29', 'Ald30_34', 'Ald35_39', 'Ald40_44', 'Ald45_49', 'Ald50_54', 'Ald55_59', 'Ald60_64', 'Ald65_69', 'Ald70_74', 'Ald75_79', 'Ald80_w']].sum(axis=1) + DeSO.loc[:,'Ald16_19']/2\n",
    "\n",
    "\n",
    "print(DeSO.crs)\n",
    "\n",
    "DeSO.to_crs(4326, inplace=True)\n",
    "print(DeSO.crs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# read employee numbers by deso zones\n",
    "# multiply the number of employees by work activity participation ratio\n",
    "\n",
    "DeSO_arb = geopandas.read_file(\n",
    "    f'C:/Synthetic_population_new/caglar/synthetic_sweden/input/deso_statistik_shp/Bef_Sysselsattning_region.shp')\n",
    "\n",
    "DeSO_arb['county'] = DeSO_arb['Deso'].str[:1]\n",
    "DeSO_arb['county'] = DeSO_arb['Deso'].str[:2].astype(int)\n",
    "\n",
    "work_participation = pd.read_csv('dbs/intermediate/work_participation.csv',index_col=0)\n",
    "\n",
    "DeSO_arb = pd.merge(DeSO_arb, work_participation, left_on=['county'], right_on=['home_county'], how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "DeSO_arb['Forvarb'] = DeSO_arb['Forvarb']*DeSO_arb['nunique']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "DeSO = pd.merge(DeSO, DeSO_arb[['Deso','Forvarb']], left_on=['Deso'], right_on=['Deso'], how=\"left\")\n",
    "\n",
    "\n",
    "DeSO.rename(columns={'Forvarb':'employee_pop'}, inplace=True )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# add deso information to the activity table\n",
    "\n",
    "gdf_act = geopandas.GeoDataFrame(df_act, geometry=geopandas.points_from_xy(df_act.cluster_lng, df_act.cluster_lat), crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "gdf_act = gdf_act.sjoin(DeSO[['Deso','geometry', 'adult_pop','employee_pop']], how=\"left\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# count the number of detected homes for each deso zones\n",
    "\n",
    "gdf_2_home = gdf_act[gdf_act.home_potential== 1]\n",
    "\n",
    "gdf_2_home= gdf_2_home.groupby('Deso')['uid'].count().reset_index().rename(columns={\"uid\":'home_count'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "gdf_2_home = pd.merge(DeSO[['Deso','adult_pop', 'employee_pop']], gdf_2_home, left_on=['Deso'], right_on=['Deso'], how=\"left\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## calculate a weight for each device\n",
    "- we use inverse probability weighting (IPW) to assign each device a weight, i.e., the reverse of the phone users’ count ratio to the DeSO zone’s actual adult population size (Wp).\n",
    "- we fit the calculated weights to the work activity participation rates by deso zone, to ensure that the work activity participation rate is accurately captured. E.g., 60% of people participate in work activity by the survey, but we detect only 20% work activity participation in the data.  By the statistic, the weight of the devices with work activity is increased and the weight of the devices with no work activity is decreased.\n",
    "- we apply weight trimming technique, where any weight above the cut-point weight (W0) is set to W0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "gdf_2_home.loc[:, 'wt_p'] = gdf_2_home.loc[:, 'adult_pop'] / gdf_2_home.loc[:, 'home_count']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#w0 = ((np.std(gdf_2_home.loc[:, 'wt_p']) / np.mean(gdf_2_home.loc[:, 'wt_p'])) ** 2 + 1) ** 0.5 * 3.5 * np.median(   gdf_2_home.loc[:, 'wt_p'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population above 17 years old:  8060839.0\n"
     ]
    }
   ],
   "source": [
    "#gdf_2_home.loc[gdf_2_home['wt_p'] > w0, 'wt_p'] = w0\n",
    "\n",
    "print('Population above 17 years old: ',np.sum(gdf_2_home['wt_p'] * gdf_2_home['home_count']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "employees = df_act['uid'][df_act.work_potential== 1].unique()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "gdf_act = gdf_act[gdf_act.home_potential == 1]\n",
    "gdf_act['work_potential'][gdf_act.uid.isin(employees)] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "gdf_act = pd.merge(gdf_act, gdf_2_home[['Deso','wt_p']], on='Deso', how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df_act = gdf_act[['uid', 'Deso', 'work_potential', 'adult_pop', 'employee_pop', 'wt_p']].sort_values(by=['Deso'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def ipf_wtp(data):\n",
    "    current_pop = data['wt_p'].sum()\n",
    "    #pop_factor = current_pop/data['adult_pop']\n",
    "    for it in range(20):\n",
    "\n",
    "        employees = data['wt_p'][data['work_potential']==1].sum() #* pop_factor\n",
    "        data['wt_p'][data['work_potential']==1] = data['wt_p'][data['work_potential']==1] * (data['employee_pop']/ employees)\n",
    "        population = data['wt_p'].sum()\n",
    "        data['wt_p'] = data['wt_p'] * (current_pop/ population)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5985 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "988413e1b62c46629a47a9d1c9174f90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit the weights to the work activity participation ratios\n",
    "\n",
    "tqdm.pandas()\n",
    "df_act = df_act.groupby('Deso').progress_apply(ipf_wtp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "w0 = ((np.std(df_act.loc[:, 'wt_p']) / np.mean(df_act.loc[:, 'wt_p'])) ** 2 + 1) ** 0.5 * 3.5 * np.median( df_act.loc[:, 'wt_p'])\n",
    "df_act.loc[df_act['wt_p'] > w0, 'wt_p'] = w0\n",
    "print('sum of all weights', df_act['wt_p'].sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# save the table\n",
    "\n",
    "df_act.to_csv(f'./dbs/intermediate/indi_weights.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## in the script 1.1, use an algorithm looking within the sweden polygone instead of spatial join with deso\n",
    "## in the script 1.1, directly remove the holidays and save the ready data\n",
    "## in the script 1.1, save the building locations as lat long as before\n",
    "## in the script 2.2, keep only people above 18\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# def ipf_wtp(data):\n",
    "#     current_pop = data['wt_p'].sum()\n",
    "#     #pop_factor = current_pop/data['adult_pop']\n",
    "#     std_eror =10\n",
    "#     while std_eror >  0.0001:\n",
    "#\n",
    "#         employees = data['wt_p'][data['work_potential']==1].sum() #* pop_factor\n",
    "#         data['wt_p'][data['work_potential']==1] = data['wt_p'][data['work_potential']==1] * (data['employee_pop']/ employees)\n",
    "#         population = data['wt_p'].sum()\n",
    "#         wt_p_old = data['wt_p']\n",
    "#\n",
    "#         data['wt_p'] = data['wt_p'] * (current_pop/ population)\n",
    "#         std_eror = np.sqrt(np.sum(np.square((wt_p_old - data['wt_p'])))/len(data))\n",
    "#\n",
    "#\n",
    "#     return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# tqdm.pandas()\n",
    "# df_act1 = df_act.groupby('Deso').progress_apply(ipf_wtp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
